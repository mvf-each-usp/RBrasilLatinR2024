---
title: "A Comunidade `R Brasil` no Telegram"
author:
  - Marcelo Ventura Freire
  - Leonardo Fernandes Nascimento
format: 
  html:
    toc: true
---

## TO-DO LIST

- inlcuir referência ao [trabalho do Trecenti sobre a R Brasil](https://blog.curso-r.com/posts/2019-09-10-rbrasil/)
- ver se tem algo que dê pra aproveitar da [minha participação sobre comunidades na SatRday 2019](https://saopaulo2019.satrdays.org/)
    - só tenho material da apresentação sobre R Markdown
    - a o que falei da R Brasil foi só falado =/
- tópicos a mencionar
    - histórico do grupo
    - interesses do grupo
    - composição do grupo
    - ??? o quê mais?
        - sugestões do Charles: 
            - Seria legal dar uma amostra das iniciativas que surgiram a partir do R-Brasil. Por exemplo, alguns grupos foram formados por pessoas interessadas em diferentes nichos, cito alguns:
                a) Web-scraping Brasil (https://t.me/+Z4d0O-ZQXsQyNDJk)
                b) Bio-R
                c) Shiny Brasil (https://t.me/shinybrasil)
                d) Dadoscope (https://revistaforum.com.br/opiniao/2019/7/10/dadoscope-novo-blog-na-forum-traz-jornalismo-de-dados-58299.html)
                e) BIT Analytics (startup criada por Tarssio e Italo com a colaboração de @Koselleck)
                f) Inúmeras colaborações em formato freelance em conjunto entre membros do grupo que mal se conhecem em pessoa (só eu já trabalhei com Tarssio, @oMarceloVentura , Marcelo Ventura -- o outro, com Neto Ferraz, @Koselleck, @FBarbalho, Arles e muitos conhecidos destas pessoas de 2017 até agora)
            - Seria legal também identificar o volume de novos membros por dia/mes/ano... se não for possível identificar essa informação com o R podíamos usar o bot do telegram mesmo. Vou procurar aqui
            - Por fim, acho que o mais importante é definir que história queremos contar. Queremos mostrar uma foto/perfil do R-Brasil? Queremos contar a História/Evolução do R-Brasil? 
            - Em qualquer gráfico que fale sobre o horário das mensagens, seria bom deixar claro de qual fuso-horário aquela mensagem se refere. Fuso horário do Brasil? UTC? Não sei na verdade qual é o fuso.
        - sugestões do Adriano Mello
            - Uma comparação com o SOF seria bem legal. O grupo aqui é bem mais acessível e receptivo em comparação.


## reciclar depois: copy-pasta do script do Trecenti

```{r}
#| eval: false
library(magrittr)

ler_html_telegram <- function(html_file) {
  # pega todas as mensagens
  divs <- xml2::read_html(html_file) %>% 
    xml2::xml_find_all("//div[@class='message default clearfix']")
  
  # nome da pessoa
  nomes <- divs %>% 
    xml2::xml_find_all("./div/div[@class='from_name']") %>% 
    xml2::xml_text() %>% 
    stringr::str_squish()
  
  # data e hora da mensagem
  data_horas <- divs %>% 
    xml2::xml_find_all("./div/div[@class='pull_right date details']") %>% 
    xml2::xml_attr("title") %>% 
    lubridate::dmy_hms()
  
  # texto da mensagem
  textos <- divs %>% 
    purrr::map(xml2::xml_find_first, "./div/div[@class='text']") %>% 
    purrr::map_chr(xml2::xml_text) %>% 
    stringr::str_squish()
  
  # retorna numa tabela
  tibble::tibble(
    data_hora = data_horas,
    nome = nomes,
    texto = textos
  )
}
```

```{r}
#| eval: false
path <- "~/Downloads/Telegram Desktop/ChatExport_17_08_2019/"
todos_arquivos <- fs::dir_ls(path, regexp = "messages")

d_msg <- purrr::map_dfr(
  todos_arquivos, 
  ler_html_telegram, 
  .id = "arquivo"
)
```

```{r}
#| eval: false
d_msg %>% 
  dplyr::count(nome, sort = TRUE) %>% 
  dplyr::mutate(prop = scales::percent(n/sum(n))) %>% 
  head(10) %>% 
  knitr::kable()
```

```{r}
#| eval: false
library(ggplot2)
d_msg %>% 
  dplyr::mutate(mes = lubridate::floor_date(data_hora, "month")) %>% 
  dplyr::count(mes) %>% 
  ggplot(aes(x = mes, y = n)) +
  geom_line() +
  geom_point() +
  theme_minimal(16)
```

```{r}
#| eval: false
d_msg %>% 
  dplyr::filter(nome != "Deleted Account") %>% 
  dplyr::mutate(nome = forcats::fct_lump(nome, 12),
                nome = as.character(nome),
                mes = lubridate::floor_date(data_hora, "month")) %>% 
  dplyr::filter(nome != "Other") %>% 
  dplyr::count(mes, nome, sort = TRUE) %>% 
  tidyr::complete(mes, nome, fill = list(n = 0)) %>% 
  ggplot(aes(x = mes, y = n)) +
  geom_line() +
  facet_wrap(~nome) +
  labs(x = "Mês", y = "Quantidade de mensagens") +
  theme_bw()
```

```{r}
#| eval: false
d_msg %>% 
  dplyr::mutate(hora = factor(lubridate::hour(data_hora))) %>% 
  ggplot(aes(x = hora)) +
  geom_bar(fill = "royalblue") +
  theme_minimal(14) +
  ggtitle("Hora das mensagens")
```

```{r}
#| eval: false
d_msg %>% 
  dplyr::mutate(wd = lubridate::wday(data_hora, label = TRUE)) %>% 
  ggplot(aes(x = wd)) +
  geom_bar(fill = "pink2") +
  theme_minimal(14) +
  ggtitle("Dia da semana das mensagens")
```

```{r}
#| eval: false
# dá pra criar funções anônimas assim ;)
# esse é um limpador bem safado que fiz em 1 min
limpar <- . %>% 
  abjutils::rm_accent() %>% 
  stringr::str_to_title() %>% 
  stringr::str_remove_all("[^a-zA-Z0-9 ]") %>% 
  stringr::str_squish()

# tirar palavras que nao quero
banned <- tidytext::get_stopwords("pt") %>% 
  dplyr::mutate(palavra = limpar(word))

cores <- viridis::viridis(10, begin = 0, end = 0.8)

d_msg %>% 
  tidytext::unnest_tokens(palavra, texto) %>% 
  dplyr::mutate(palavra = limpar(palavra)) %>% 
  dplyr::anti_join(banned, "palavra") %>% 
  dplyr::count(palavra, sort = TRUE) %>% 
  with(wordcloud::wordcloud(
    palavra, n, scale = c(5, .1), 
    min.freq = 80, random.order = FALSE,
    colors = cores
  ))
```



